{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US Population Segmentation with Amazon Sagemaker\n",
    "\n",
    "In this Notebook we will use two data science algorithms in the Sagemaker API.\n",
    "\n",
    "PCA to reduce the number of dimensions in a dataset.\n",
    "\n",
    "K-Means Clustering to provide insight into the natural grouping of data.\n",
    "\n",
    "More information can be found here:  https://aws.amazon.com/blogs/machine-learning/analyze-us-census-data-for-population-segmentation-using-amazon-sagemaker/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "matplotlib.style.use(\"ggplot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Loading the dataset\n",
    "\n",
    "We have previously downloaded and stored the data in a public S3 bucket that you can access. You can use the Python SDK to interact with AWS using a Boto3 client.\n",
    "\n",
    "First, we start the s3 client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "data_bucket_name = \"aws-ml-blog-sagemaker-census-segmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = s3_client.list_objects(Bucket=data_bucket_name)\n",
    "file = []\n",
    "for contents in obj_list[\"Contents\"]:\n",
    "    file.append(contents[\"Key\"])\n",
    "print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = file[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3_client.get_object(Bucket=data_bucket_name, Key=file_data)\n",
    "response_body = response[\"Body\"].read()\n",
    "counties = pd.read_csv(io.BytesIO(response_body), header=0, delimiter=\",\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 2: Exploratory data analysis EDA - Data cleaning and exploration\n",
    "\n",
    "### a. Cleaning the data\n",
    "\n",
    "We can do simple data cleaning and processing right in our notebook instance, using the compute instance of the notebook to execute these computations.\n",
    "\n",
    "How much data are we working with?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties.dropna(inplace=True)\n",
    "counties.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties.index = counties[\"State\"] + \"-\" + counties[\"County\"]\n",
    "counties.head()\n",
    "drop = [\"CensusId\", \"State\", \"County\"]\n",
    "counties.drop(drop, axis=1, inplace=True)\n",
    "counties.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Visualizing the data\n",
    "\n",
    "Now we have a dataset with a mix of numerical and categorical columns. We can visualize the data for some of our numerical columns and see what the distribution looks like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in [\"Professional\", \"Service\", \"Office\"]:\n",
    "    ax = plt.subplots(figsize=(6, 3))\n",
    "    ax = sns.distplot(counties[a])\n",
    "    title = \"Histogram of \" + a\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### c. Feature engineering\n",
    "\n",
    "Data Scaling- We need to standardize the scaling of the numerical columns in order to use any distance based analytical methods so that we can compare the relative distances between different feature columns. We can use minmaxscaler to transform the numerical columns so that they also fall between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "counties_scaled = pd.DataFrame(scaler.fit_transform(counties))\n",
    "counties_scaled.columns = counties.columns\n",
    "counties_scaled.index = counties.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_scaled.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 3: Data modelling\n",
    "\n",
    "### a. Dimensionality reduction\n",
    "\n",
    "We will be using principal component analysis (PCA) to reduce the dimensionality of our data. This method decomposes the data matrix into features that are orthogonal with each other. The resultant orthogonal features are linear combinations of the original feature set. You can think of this method as taking many features and combining similar or redundant features together to form a new, smaller feature set.\n",
    "\n",
    "We can reduce dimensionality with the built-in Amazon SageMaker algorithm for PCA.\n",
    "\n",
    "We first import and call an instance of the PCA SageMaker model. Then we specify different parameters of the model. These can be resource configuration parameters, such as how many instances to use during training, or what type of instances to use. Or they can be model computation hyperparameters, such as how many components to use when performing PCA. Documentation on the PCA model can be found here: http://sagemaker.readthedocs.io/en/latest/pca.html\n",
    "\n",
    "Be sure to specify the bucket name for a bucket in your account where you want SageMaker model parameters to be stored. Note that the bucket must be in the same region as this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import PCA\n",
    "\n",
    "num_components = 33\n",
    "\n",
    "pca_SM = PCA(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=\"s3://\" + bucket + \"/counties/\",\n",
    "    num_components=num_components,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = counties_scaled.values.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "pca_SM.fit(pca_SM.record_set(train_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### b. Accessing the PCA model attributes\n",
    "\n",
    "After the model is created, we can also access the underlying model parameters.\n",
    "\n",
    "Now that the training job is complete, you can find the job under Jobs in the Training subsection in the Amazon SageMaker console.\n",
    "\n",
    "Model artifacts are stored in Amazon S3 after they have been trained. This is the same model artifact that is used to deploy a trained model using Amazon SageMaker. Since many of the Amazon SageMaker algorithms use MXNet for computational speed, the model artifact is stored as an ND array. For an output path that was specified during the training call, the model resides in <training_job_name>/output/model.tar.gz file, which is a TAR archive file compressed with GNU zip (gzip) compression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = pca_SM.latest_training_job.name\n",
    "model_key = \"counties/\" + job_name + \"/output/model.tar.gz\"\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).download_file(model_key, \"model.tar.gz\")\n",
    "os.system(\"tar -zxvf model.tar.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mxnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model_params = mx.ndarray.load(\"model_algo-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Three groups of model parameters are contained within the PCA model.\n",
    "\n",
    "mean: is optional and is only available if the “subtract_mean” hyperparameter is true when calling the training step from the original PCA SageMaker function.\n",
    "\n",
    "v: contains the principal components (same as ‘components_’ in the sklearn PCA model).\n",
    "\n",
    "s: the singular values of the components for the PCA transformation. This does not exactly give the % variance from the original feature space, but can give the % variance from the projected feature space.\n",
    "\n",
    "explained-variance-ratio ~= square(s) / sum(square(s))\n",
    "\n",
    "To calculate the exact explained-variance-ratio vector if needed, it simply requires saving the sum of squares of the original data (call that N) and computing explained-variance-ratio = square(s) / N.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.DataFrame(pca_model_params[\"s\"].asnumpy())\n",
    "v = pd.DataFrame(pca_model_params[\"v\"].asnumpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can now calculate the variance explained by the largest n components that we want to keep. For this example, let's take the top 5 components.\n",
    "\n",
    "We can see that the largest 5 components explain ~72% of the total variance in our dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.iloc[28:, :].apply(lambda x: x * x).sum() / s.apply(lambda x: x * x).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have decided to keep the top 5 components, we can take only the 5 largest components from our original s and v matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_5 = s.iloc[28:, :]\n",
    "v_5 = v.iloc[:, 28:]\n",
    "v_5.columns = [0, 1, 2, 3, 4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can now examine the makeup of each PCA component based on the weightings of the original features that are included in the component. For example, the following code shows the first component. We can see that this component describes an attribute of a county that has high poverty and unemployment, low income and income per capita, and high Hispanic/Black population and low White population.\n",
    "\n",
    "Note that this is v_5[4] or last component of the list of components in v_5, but is actually the largest component because the components are ordered from smallest to largest. So v_5[0] would be the smallest component. Similarly, change the value of component_num to cycle through the makeup of each component.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_num = 1\n",
    "\n",
    "first_comp = v_5[5 - component_num]\n",
    "comps = pd.DataFrame(\n",
    "    list(zip(first_comp, counties_scaled.columns)), columns=[\"weights\", \"features\"]\n",
    ")\n",
    "comps[\"abs_weights\"] = comps[\"weights\"].apply(lambda x: np.abs(x))\n",
    "ax = sns.barplot(\n",
    "    data=comps.sort_values(\"abs_weights\", ascending=False).head(10),\n",
    "    x=\"weights\",\n",
    "    y=\"features\",\n",
    "    palette=\"Blues_d\",\n",
    ")\n",
    "ax.set_title(\"PCA Component Makeup: #\" + str(component_num))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can go through and examine the makeup of each PCA components and try to understand what the key positive and negative attributes are for each component. The following code names the components, but feel free to change them as you gain insight into the unique makeup of each component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_list = [\"comp_1\", \"comp_2\", \"comp_3\", \"comp_4\", \"comp_5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### c. Deploying the PCA model\n",
    "\n",
    "We can now deploy this model endpoint and use it to make predictions. This model is now live and hosted on an instance_type that we specify.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_predictor = pca_SM.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also pass our original dataset to the model so that we can transform the data using the model we created. Then we can take the largest 5 components and this will reduce the dimensionality of our data from 34 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = pca_predictor.predict(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_transformed = pd.DataFrame()\n",
    "for a in result:\n",
    "    b = a.label[\"projection\"].float32_tensor.values\n",
    "    counties_transformed = counties_transformed.append([list(b)])\n",
    "counties_transformed.index = counties_scaled.index\n",
    "counties_transformed = counties_transformed.iloc[:, 28:]\n",
    "counties_transformed.columns = PCA_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have created a dataset where each county is described by the 5 principle components that we analyzed earlier. Each of these 5 components is a linear combination of the original feature space. We can interpret each of these 5 components by analyzing the makeup of the component shown previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_transformed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### d. Population segmentation using unsupervised clustering\n",
    "\n",
    "Now, we’ll use the Kmeans algorithm to segment the population of counties by the 5 PCA attributes we have created. Kmeans is a clustering algorithm that identifies clusters of similar counties based on their attributes. Since we have ~3000 counties and 34 attributes in our original dataset, the large feature space may have made it difficult to cluster the counties effectively. Instead, we have reduced the feature space to 5 PCA components, and we’ll cluster on this transformed dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = counties_transformed.values.astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we call and define the hyperparameters of our KMeans model as we have done with our PCA model. The Kmeans algorithm allows the user to specify how many clusters to identify. In this instance, let's try to find the top 7 clusters from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import KMeans\n",
    "\n",
    "num_clusters = 7\n",
    "kmeans = KMeans(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    output_path=\"s3://\" + bucket + \"/counties/\",\n",
    "    k=num_clusters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train the model on our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kmeans.fit(kmeans.record_set(train_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we deploy the model and we can pass in the original training set to get the labels for each entry. This will give us which cluster each county belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kmeans_predictor = kmeans.deploy(initial_instance_count=1, instance_type=\"ml.t2.medium\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = kmeans_predictor.predict(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the breakdown of cluster counts and the distribution of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_labels = [r.label[\"closest_cluster\"].float32_tensor.values[0] for r in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cluster_labels)[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ax = plt.subplots(figsize=(6, 3))\n",
    "ax = sns.distplot(cluster_labels, kde=False)\n",
    "title = \"Histogram of Cluster Counts\"\n",
    "ax.set_title(title, fontsize=12)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "However, to improve explainability, we need to access the underlying model to get the cluster centers. These centers will help describe which features characterize each cluster.\n",
    "\n",
    "## Step 4: Drawing conclusions from our modelling\n",
    "\n",
    "Explaining the result of the modelling is an important step in making use of our analysis. By combining PCA and Kmeans, and the information contained in the model attributes within an Amazon SageMaker trained model, we can form concrete conclusions based on the data.\n",
    "\n",
    "\n",
    "### a. Accessing the KMeans model attributes\n",
    "\n",
    "First, we will go into the bucket where the kmeans model is stored and extract it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = kmeans.latest_training_job.name\n",
    "model_key = \"counties/\" + job_name + \"/output/model.tar.gz\"\n",
    "\n",
    "boto3.resource(\"s3\").Bucket(bucket).download_file(model_key, \"model.tar.gz\")\n",
    "os.system(\"tar -zxvf model.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmeans_model_params = mx.ndarray.load(\"model_algo-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "There is 1 set of model parameters that is contained within the KMeans model.\n",
    "\n",
    "Cluster Centroid Locations: The location of the centers of each cluster identified by the Kmeans algorithm. The cluster location is given in our PCA transformed space with 5 components, since we passed the transformed PCA data into the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_centroids = pd.DataFrame(Kmeans_model_params[0].asnumpy())\n",
    "cluster_centroids.columns = counties_transformed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cluster_centroids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We can plot a heatmap of the centroids and their location in the transformed feature space. This gives us insight into what characteristics define each cluster. Often with unsupervised learning, results are hard to interpret. This is one way to make use of the results of PCA plus clustering techniques together. Since we were able to examine the makeup of each PCA component, we can understand what each centroid represents in terms of the PCA components that we intepreted previously.\n",
    "\n",
    "For example, we can see that cluster 1 has the highest value in the \"Construction & Commuters\" attribute while it has the lowest value in the \"Self Employment/Public Workers\" attribute compared with other clusters. Similarly, cluster 4 has high values in \"Construction & Commuters,\" \"High Income/Professional & Office Workers,\" and \"Self Employment/Public Workers.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "ax = sns.heatmap(cluster_centroids.T, cmap=\"YlGnBu\")\n",
    "ax.set_xlabel(\"Cluster\")\n",
    "plt.yticks(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "ax.set_title(\"Attribute Value by Centroid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also map the cluster labels back to each individual county and examine which counties were naturally grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_transformed[\"labels\"] = list(map(int, cluster_labels))\n",
    "counties_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = counties_transformed[counties_transformed[\"labels\"] == 1]\n",
    "cluster.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Conclusion\n",
    "\n",
    "You have just walked through a data science workflow for unsupervised learning, specifically clustering a dataset using KMeans after reducing the dimensionality using PCA. By accessing the underlying models created within Amazon SageMaker, we were able to improve the explainability of our modelling and draw actionable conclusions. Using these techniques, we have been able to better understand the essential characteristics of different counties in the US and segment the electorate into groupings accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because endpoints are persistent, let’s delete our endpoints now that we’re done to avoid any excess charges on our AWS bill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(pca_predictor.endpoint)\n",
    "sagemaker.Session().delete_endpoint(kmeans_predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-2:712779665605:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
